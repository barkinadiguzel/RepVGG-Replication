# ğŸš© RepVGG-Replication PyTorch Implementation

This repository contains a replication of **RepVGG** using PyTorch. The goal is to build a **VGG-style CNN backbone** with **training-time multi-branch residual blocks** and **structural re-parameterization** for efficient inference.

- Implemented **RepVGG** with plain convolutional backbone and training-time multi-branch residual blocks.  
- Architecture:  
**Conv â†’ ResidualBlock â†’ ... â†’ ResidualBlock â†’ BN â†’ ReLU â†’ AvgPool â†’ Flatten â†’ FC**  
**Paper**: [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)

---
## ğŸ–¼ Overview â€“ RepVGG with Structural Re-param

![Figure 2-4](images/figmix.jpg)  

**Figure 2:** Sketch of a RepVGG stage. Each stage has multiple blocks; the first layer downsamples via stride-2 convolution. During training, each block has a 3Ã—3 conv, a 1Ã—1 conv, and an identity branch (if dims match), forming a multi-branch structure. For inference, these branches are fused into a single 3Ã—3 conv for efficiency.

**Figure 3:** Memory and computation comparison. Plain ConvNet topology reduces peak memory usage and allows more parallelism, while training-time multi-branch blocks introduce extra memory overhead. After branch fusion, inference latency and memory usage remain minimal.

**Figure 4:** Stage-wise layout of RepVGG. The network has 5 stages with configurable depth and width. Later stages have wider channels for richer features, and depth is concentrated in the penultimate stage for optimal performance.

**Model Overview:**  
RepVGG is a VGG-style plain backbone with training-time multi-branch blocks. Branches are fused for efficient inference while retaining high accuracy. The model balances **simplicity, speed, and memory efficiency** with the **benefit of ensemble-like learning** during training.

---

## ğŸ— Project Structure

```bash
RepVGG-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py
â”‚   â”‚   â”œâ”€â”€ residual_block.py        # RepVGGBlock with train-time multi-branch
â”‚   â”‚   â”œâ”€â”€ fuse_layers.py           # Kernel/bias fusion helper functions
â”‚   â”‚   â”œâ”€â”€ shortcut_layer.py
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py
â”‚   â”‚   â””â”€â”€ fc_layer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ repvgg_model.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
